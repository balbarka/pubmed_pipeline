{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0015194f-f390-4b43-a01c-7aabba831282",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Article Chunking with `unstructured`\n",
    "\n",
    "We are planning to use the [unstructured](https://unstructured.io/) for our primary chunking strategy. We are going to use this for the actual body content and it is common to change the arguments of the unstructured [partitioning](https://docs.unstructured.io/open-source/core-functionality/partitioning) functions upon future iterations where we are improving our Dataset curation for pre-training or fine-tuning or our chunking strategy for our VS index.\n",
    "\n",
    "**NOTE**: Since we are working with XML data we are going to use the [partition-xml](https://docs.unstructured.io/open-source/core-functionality/partitioning#partition-xml) function. There are many libraries out there that can make use of the xml tags we left in our body column and they can excluded easily with regex or opensource xml parsing library. Thus, we left the xml in the body to allow for discovery of new / different parsing strategies in the future.\n",
    "\n",
    "**NOTE**: YES. We could have used [partition-xml](https://docs.unstructured.io/open-source/core-functionality/partitioning#partition-xml) function to parse from file instead of from the `curated_articles` delta table. Similar to the above note, we did this to make future iterative improvements faster as reading text from file in blob storage has a much larger I/O preformance cost. This was a deliberate architecture decision for future enhancements, not just to conform to a [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)... although we are doing that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc1d035-6e4e-4a37-8c24-3008a16be666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install unstructured\n",
    "%pip install databricks_genai_inference\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8879b71-15ef-4709-b61d-18091f84d0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.xml import partition_xml\n",
    "\n",
    "# We'll just collect a couple body records and parse locally to understand that \n",
    "xml_bodies = spark.sql(\"SELECT body FROM `pubmed-pipeline`.curated.articles_xml limit 5\").collect()\n",
    "# TODO, add xlink back from original source\n",
    "xml_body = '<root xmlns:xlink=\"http://www.w3.org/1999/xlink\">'+ \\\n",
    "           xml_bodies[0][0] + \\\n",
    "           '</root>'\n",
    "\n",
    "print(xml_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3143cb65-abe7-4db7-9f93-4e5ed489c393",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "?partition_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7f2976-ab3f-44d6-b0b8-e3cf7b7ecca5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.xml import partition_xml\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a datetime object representing the last modified time\n",
    "# TODO: update to pull from source (or determine safe to exclude)\n",
    "last_modified_time = datetime(2023, 5, 29, 15, 30)\n",
    "\n",
    "#TODO: check if we have multi-language sources\n",
    "\n",
    "body_parts = partition_xml(text=xml_body,\n",
    "                           xml_keep_tags = False,\n",
    "                           encoding='utf-8',\n",
    "                           include_metadata=False,\n",
    "                           languages=['eng',],\n",
    "                           date_from_file_object=None,\n",
    "                           chunking_strategy='by_title',\n",
    "                           multipage_sections=True,\n",
    "                           combine_text_under_n_chars=275,\n",
    "                           new_after_n_chars=500,\n",
    "                           max_characters=520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849fbd61-cec9-402d-bc7f-3a64cd39b7cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's confirm that the chunks lengths distributed as expected\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "lengths = [len(element.text) for element in body_parts]\n",
    "lengths.sort()\n",
    "lengths\n",
    "\n",
    "# Calculate the cumulative probabilities\n",
    "cumulative_probs = np.arange(1, len(lengths) + 1) / len(lengths)\n",
    "\n",
    "# Plot the empirical distribution\n",
    "plt.plot(lengths, cumulative_probs, marker='o')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Empirical Distribution of Chunk Lengths')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ab5173-01b4-4b15-b5fe-4fa7ebffb9ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "This was achieved with minimal arguments / modifications. However, we can see that we still have some chucks that have less than 100 characters. We'll have to make a chunking strategy decision to either drop those chunks or force into larger chunks with a follow on process. We'll choose the former which will result in the following for our chunks of a single body example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d3d820-26ba-4c08-863d-ac0b3a6980b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunks = [e.text for e in body_parts if len(e.text)]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c788588d-95ae-4a48-bd55-b4476884b798",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "It is also common to use LLMs to improve the chunks including the use of summarization of these chunks. That will be left for a future improvement, but here is an example using a databricks foundation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ead6b32-627f-4639-b56d-a4169ff0f732",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_genai_inference import ChatCompletion\n",
    "import os\n",
    "\n",
    "os.environ['DATABRICKS_HOST'] = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "os.environ['DATABRICKS_TOKEN'] = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "\n",
    "def dbrx_summarize(chunk: str):\n",
    "    return ChatCompletion.create(model=\"databricks-dbrx-instruct\",\n",
    "                                 messages=[{\"role\": \"system\", \"content\": \"You are a researcher who wants to summarize user text without losing technical detail. Respond to the users with only a summary of the content they provide. Try to summarize with not more than 500 characters.\"},\n",
    "                                          {\"role\": \"user\",\"content\": chunk}],\n",
    "                                max_tokens=600).message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdf4eb4-73dc-47db-8dcc-ab229da134ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summarized_chunks = [dbrx_summarize(c) for c in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7406b86-170b-4f7f-b31f-feafc0826c44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summarized_chunks[:3]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "xx_Unstructured_Chunking_Discussion",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
