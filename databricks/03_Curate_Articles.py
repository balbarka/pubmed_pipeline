# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC # Curate Articles
# MAGIC
# MAGIC This notebook will run first a first level parsing and save article content into a delta table.
# MAGIC
# MAGIC Append new articles in curated_articles table. If a document will be updated, a new AccessionID/PMID will be generated by PMC and the old one will be retracted.
# MAGIC
# MAGIC *TODO*: Remove retracted articles

# COMMAND ----------

dbutils.widgets.dropdown(name="FILE_TYPE", defaultValue="xml", choices=["xml", "text"])
FILE_TYPE = dbutils.widgets.get("FILE_TYPE")
dbutils.widgets.dropdown(name="INSPECT_CURATED", defaultValue="true", choices=["true", "false"])

# COMMAND ----------

# MAGIC %run ./_resources/pubmed_pipeline_config $RESET_ALL_DATA=false $DISPLAY_CONFIGS=true

# COMMAND ----------

# MAGIC %run ./_resources/pubmed_central_utils

# COMMAND ----------

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, IntegerType, DoubleType, MapType, StructType, StructField

# Define the UDF
def article_parse_xml(accession_id: str, volume_path: str) -> tuple:
    from bs4 import BeautifulSoup
    with open(volume_path, 'r') as file:
        article = BeautifulSoup(file.read(), 'xml').find('article')
    return (accession_id, volume_path,
        {str(k):str(v) for k,v in article.attrs.items()},
        str(article.find('front')),
        str(article.find('body')),
        str(article.find('floats-group')),
        str(article.find('back')),
        str(article.find('processing-meta')))

# Register the UDF
article_parse_xml_udf = udf(article_parse_xml, 
                       returnType=StructType([
                           StructField("AccessionID",  StringType(), nullable=False),
                           StructField("volume_path",  StringType(), nullable=False),
                           StructField("attrs",        MapType(StringType(), StringType()), nullable=False),
                           StructField("front",         StringType(), nullable=False),
                           StructField("body",         StringType(), nullable=False),
                           StructField("floats_group", StringType(), nullable=False),
                           StructField("back", StringType(), nullable=False),
                           StructField("processing_metadata", StringType(), nullable=False)]))

# COMMAND ----------

parsed_articles = pubmed.raw_metadata.df.filter('status="DOWNLOADED"') \
                        .join(pubmed.curated_articles.df, 'AccessionID', 'leftanti') \
                        .withColumn('parsed_struct', article_parse_xml_udf("AccessionID", "volume_path")) \
                        .select('parsed_struct.AccessionID',
                                'ETag',
                                'LastUpdated',
                                'PMID',
                                'parsed_struct.attrs',
                                'parsed_struct.front',
                                'parsed_struct.body',
                                'parsed_struct.floats_group',
                                'parsed_struct.back',
                                'parsed_struct.processing_metadata',
                                '_ingestion_timestamp',
                                'volume_path')

merged_parsed_articles = pubmed.curated_articles.dt.alias('tgt') \
                               .merge(parsed_articles.alias('src'), "src.AccessionID = tgt.AccessionID") \
                               .whenMatchedUpdateAll() \
                               .whenNotMatchedInsertAll() \
                               .execute()

# COMMAND ----------

if dbutils.widgets.get("INSPECT_CURATED") == 'true':
    dat = spark.sql(f"SELECT * FROM {pubmed.curated_articles.name}")
    display(dat)
